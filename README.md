<div align="center">
<a name="start-anchor"></a>
  <img src="fig\title_logo.jpg" alt="logo" width="600" height="auto" />
  <br/>
<b>We collect datasets and other popular model codes.</b>
  <br/>
<b>We give a series of instructions and demo files.</b>
  <br/>
<b>We release all code in this code repository.</b>
</div>
<div align="center">
<b>Authors: Chunyu Zhao, Wentao Mu, Xian Zhou, Wenbo Liu, Fei Yan, Tao Deng*</b>
</div>

<div align="center">
  <img src="fig/demo-example1.gif" alt="BDDA-1" width="200" height="auto" />
  <img src="fig/demo-example2.gif" alt="BDDA-2" width="200" height="auto" />
  <img src="fig/demo-example3.gif" alt="BDDA-2" width="200" height="auto" />
</div>

## ğŸ”¥Update

- **2024/12/10**: ***Our paper is accepted by AAAIğŸ‰ğŸ‰ğŸ‰***. <a href="https://arxiv.org/pdf/2502.16214" target="_blank"><img src="fig/arxiv_.png" alt="arxiv" width="50" height="auto" /></a>
  
- **2024/11/08**: ***Update supplementary materials***. [Details](supplementary.md)

- **We release all the runnable code. **
  
- **Comparison of Runtime and GPU memory.** 
  
- **2024/10/23**: We release the uniform saliency dataset loader. You can simply use it by `from utils.datasets import build_dataset`.

- **2024/07/25**: How to use our model ($SalM^2$).
    - How to train:  [command](#Run-train ) & [script](#Run-train )
    - How to test:  [command](#Run-test ) & [script](#Run-test )
  - compare model
    - Static prediction model:  [command](#Run-train ) & [script](#Run-train )
    - Dynamic prediction model:  [command](#Run-train ) & [script](#Run-train )

- **2024/07/24**: All the code and models are completed.

- **2024/07/05**: We collect the possible datasets to use, and make a uniform dataloader.

- **2024/06/14**: Our model is proposed !

## ğŸ’¬Motivation

â€‹	**(1) Using semantic information to guide driver attention.**
<div align="center">
<img src="fig\Motivation1.png" width="auto" height="auto" />
</div>
<b>Solution:</b> We propose a dual-branch network that separately extracts semantic information and image information. The semantic information is used to guide the image information at the deepest level of image feature extraction.

â€‹	**(2) Reducing model parameters and computational complexity.**
<div align="center">
<img src="fig\para_s.png" style="zoom: 100%;"><img src="fig\flops_s.png" style="zoom: 100%;">
</div>
<b>Solution:</b> We develop a highly lightweight saliency prediction network based on the latest Mamba framework, with only <b>0.0785M</b> (<b>88% reduction compared to SOTA</b>) parameters and <b>4.45G FLOPs</b> (<b>37% reduction compared to SOTA</b>).

## âš¡Proposed Model

we propose a saliency mamba model, named $SalM^2$ that uses "Top-down" driving scene semantic information to guide "Bottom-up" driving scene image information to simulate human drivers' attention allocation. 

<img src="fig\overview.png" style="zoom: 100%;">

## ğŸ“–Datasets
<div align="center">
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Train (video/frame)</th>
      <th>Valid (video/frame)</th>
      <th>Test (video/frame)</th>
      <th>Dataset example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>TrafficGaze</td>
      <td>49080</td>
      <td>6655</td>
      <td>19135</td>
      <td><img src="fig/TrafficGaze-example.gif" alt="BDDA-3" style="zoom:100%;" /></td>
    </tr>
    <tr>
      <td>DrFixD-rainy</td>
      <td>52291</td>
      <td>9816</td>
      <td>19154</td>
      <td><img src="fig/DrFixD-rainy-example.gif" alt="BDDA-1" style="zoom:100%;" /></td>
    </tr>
    <tr>
      <td>BDDA</td>
      <td>286251</td>
      <td>63036</td>
      <td>93260</td>
      <td><img src="fig/BDDA-example.gif" alt="BDDA-0" style="zoom:100%;" /></td>
    </tr>
  </tbody>
</table>
</div>
ã€noteã€‘ For all datasets we will provide our download link with the official link. Please choose according to your needs.

> (1) **TrafficGaze**: This dataset we uploaded in [link](www.baidu.com "Download TrafficGaze"). We crop 5 frames before and after each video. Official web in [link](https://github.com/taodeng/CDNN-traffic-saliency "Official Traffic_Gaze").
>
> (2) **DrFixD-rainy**: This dataset we uploaded in [link](www.baidu.com "Download DrFixD-rainy"). We crop 5 frames before and after each video. Official web in [link](https://github.com/taodeng/DrFixD-rainy "Official DrFixD-rainy").
>
> (3) **BDDA**: This dataset we uploaded in [link](www.baidu.com "Download BDDA"). Some camera videos and gazemap videos frame rate inconsistency, we have matched and cropped them. Some camera videos do not correspond to gazemap videos, we have filtered them. Official web in [link](https://deepdrive.berkeley.edu/ "Official BDDA").

<div align="center">
<table style="width: 100%; table-layout: auto;">
  <tr>
    <th>TrafficGaze</th>
    <th>DrFixD-rainy</th>
    <th>BDDA</th>
  </tr>
  <tr>
    <td>
      ./TrafficGaze<br>
      &emsp;&emsp;|â€”â€”fixdata<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”fixdata1.mat<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”fixdata2.mat<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”fixdata16.mat<br>
      &emsp;&emsp;|â€”â€”trafficframe<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”01<br>
      &emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|â€”â€”000001.jpg<br>
      &emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”02<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”16<br>
      &emsp;&emsp;|â€”â€”test.json<br>
      &emsp;&emsp;|â€”â€”train.json<br>
      &emsp;&emsp;|â€”â€”valid.json
    </td>
    <td>
      ./DrFixD-rainy<br>
      &emsp;&emsp;|â€”â€”fixdata<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”fixdata1.mat<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”fixdata2.mat<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”fixdata16.mat<br>
      &emsp;&emsp;|â€”â€”trafficframe<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”01<br>
      &emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|â€”â€”000001.jpg<br>
      &emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”02<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”16<br>
      &emsp;&emsp;|â€”â€”test.json<br>
      &emsp;&emsp;|â€”â€”train.json<br>
      &emsp;&emsp;|â€”â€”valid.json
    </td>
    <td>
      ./BDDA<br>
      &emsp;&emsp;|â€”â€”camera_frames<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”0001<br>
      &emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|â€”â€”0001.jpg<br>
      &emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”0002<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”2017<br>
      &emsp;&emsp;|â€”â€”gazemap_frames<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”0001<br>
      &emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|â€”â€”0001.jpg<br>
      &emsp;&emsp;|&emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”0002<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€” ... ...<br>
      &emsp;&emsp;|&emsp;&emsp;|â€”â€”2017<br>
      &emsp;&emsp;|â€”â€”test.json<br>
      &emsp;&emsp;|â€”â€”train.json<br>
      &emsp;&emsp;|â€”â€”valid.json
    </td>
  </tr>
</table>
</div>

## ğŸ› ï¸ Deployment **[ğŸ”](#ğŸ”¥Update)**

### 	Run train 

â€‹	ğŸ‘‰*If you wish to train with our model, please use the proceeding steps below.*

1. Train our model.  You can use `--category` to switch datasets, which include `TrafficGaze`, `DrFixD-rainy`, `BDDA`. `--b` sets batch size, `--g` sets id of cuda.

```python
python train.py --network salmm --b 32 --g 0 --category xxx --root xxx
```

â€‹	2. Train compare model. If the model is a *<u>**static prediction method**</u>*, run the following command.

```python
python train.py --network xxx --b 32 --g 1 --category xxx --root xxx
```

â€‹	3. Train compare model. If the model is a ***<u>dynamic prediction method</u>***, set `--seq_len` and run the following command.

```python
python train.py --network xxx --b 32 --seq_len 6 --g 2 --category xxx --root xxx
```

### 	Run test 

â€‹	ğŸ‘‰*If you wish to make predictions directly using our model results, you can do so using the proceeding steps.*

â€‹	1. Test our model.

â€‹		(a) You need to download our trained model file in [link](www.baidu.com "Download salmm model.tar") and put it to the specified folder path.

â€‹		(b) You should use `--category` to switch datasets, which include `TrafficGaze`, `DrFixD-rainy`, `BDDA`. Run the following command.

```python
python evaluate-metrics.py --network salmm --b 1 --g 0 --category xxx --root xxx --test_weight xxx
```

â€‹	ğŸ‘‰If you are unable to adapt your environment for other reasons, you can also download our predictions directly.

â€‹	2. Download prediction results.

<div align="center">
<table>
  <thead>
    <tr>
      <th><i>$SalM^2$</i> for <i>TrafficGaze</i></th>
      <th><i>$SalM^2$</i> for <i>DrFixD-rainy</i></th>
      <th><i>$SalM^2$</i> for <i>BDDA</i></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="http://www.baidu.com" title="Prediction TrafficGaze">The prediction results link</a></td>
      <td><a href="http://www.baidu.com" title="Prediction DrFixD-rainy">The prediction results link</a></td>
      <td><a href="http://www.baidu.com" title="Prediction BDDA">The prediction results link</a></td>
    </tr>
  </tbody>
</table>
</div>

## ğŸš€ Live Demo **[ğŸ”](#ğŸ”¥Update)**

<div align="center">
  <img src="fig/demo-example1.gif" alt="BDDA-1" width="230" height="auto" />
  <img src="fig/demo-example2.gif" alt="BDDA-2" width="230" height="auto" />
  <img src="fig/demo-example3.gif" alt="BDDA-3" width="230" height="auto" />
</div>


## âœ¨ Downstream Tasks **[ğŸ”](#ğŸ”¥Update)**

*Some interesting downstream tasks are shown here, and our work will be of significant research interest.*

- ***Saliency object detection***: `saliency map` â†’ **Guide** â†’ `object detection`
<div align="center">
  <img src="fig/downstream_task1.png" alt="B" style="zoom:100%;" />
</div>

- ***Event recognition***: `saliency map` â†’ **Guide** â†’ `event recognition`

<div align="center">
  <img src="fig/downstream_task2.png" alt="B" style="zoom:100%;" />
</div>

- ***Other downstream tasks***......

## â­ï¸ Cite **[ğŸ”](#ğŸ”¥Update)**

[è·³è½¬åˆ°è‡ªå®šä¹‰é”šç‚¹](#start-anchor)

If you find this repository useful, please use the following BibTeX entry for citation.

```python
@inproceedings{zhao2025salmmamba,
  title={SalMÂ²: An Extremely Lightweight Saliency Mamba Model for Real-Time Cognitive Awareness of Driver Attention},
  author={Zhao, Chunyu; Wentao Mu; Xian Zhou; Wenbo Liu; Fei Yan; Tao Deng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={},
  number={},
  pages={},
  year={2025}
}
```
